                              ____________

                               A2 WRITEUP
                              ____________





GROUP MEMBERS
-------------

  - Member 1: Angie Chen chen7313
  - Member 2: Jingli Kong kong0147

  Up to 2 people may collaborate on this assignment. Write names/x.500
  above. If working alone, leave off Member 2.

  ONLY ONE GROUP MEMBER SHOULD SUBMIT TO GRADESCOPE THEN ADD THEIR
  PARTNER ACCORDING TO INSTRUCTIONS IN THE ASSIGNMENT WEB PAGE.


Problem 1: heat_mpi
===================

heat_mpi Timing Table
~~~~~~~~~~~~~~~~~~~~~

  Fill in the following table on measuring the performance of your
  `heat_mpi' program on MSI's Mesabi machine. Replace 00.00 entries with
  your actual run times. You can use the provided `heat-slurm.sh' script
  to ease this task. Submit it using `sbatch heat-slurm.sh' and extract
  the lines marked `runtime:'.

  ---------------------------------------------
                 Width         
   Procs   6400  	25600      102400	204800
  ---------------------------------------------
       1  8.77   	16.55   	8.34 	7.06
       2  5.19	4.82   	5.01 	5.29
       4  4.52  	4.84	  	5.02 	5.17
       8  4.76  	5.03   	6.92 	5.75
      10  6.38	5.34  		5.50  	5.50 
      16  4.73  	5.01   	5.23 	5.41
      32  5.88 	5.43  		6.03	6.17
      64  5.25 	5.34  		7.14	11.01
     128  5.24 	5.58  		10.19	10.33
  ---------------------------------------------
heat_mpi Discussion Questions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Analyze your table of results and answer the following questions.

  1. Did using more processors result in speedups?
Using more processors resulted in speedups to a point. For example, having 2 processors instead of a singular processor resulted in a speedup in all listed problem sizes. To compare a specific instance, the timing for 1 processor during a 102400 problem size was 8.34. In contrast, the timing for 2 processors of the same problem size was 5.01, notably faster than having a singular processor. There are caveats in that after 16 processors are used, the runtime starts to increase even with the same problem size. 


  2. Describe any trends or anomalies you see in the timings and
     speculate on their causes - e.g. was there are a steady increase in
     runtimes, steady decrease, or jagged changes in timing?
A trend that I noticed was that at first, having more processors led to a sharp, then lower decline in processor speed. Additionally, after a certain point, increasing the amount of processors started to increase the processing time (though it overall remained faster than using only a single processor). I would speculate that this is due to the overhead required with having more processors. At a certain point, adding more processors to the problem set only adds more overhead needed to communicate between all of the processors, especially if the problem size doesn’t increase. Since overall having more processors allows for faster computation of within the problem size, they all remain faster than having a single processor. 

Another trend that I noticed was that 10 processors seemed to perform worse than the other processors, which were all a power of two, specifically for the lower problem sizes (6400 and 25600). My guess is that 10 is not as easily divisible, so the problem size might not divide as easily. This overall leads to a longer processing time than for the number of processors that are a power of two. 
 

  3. Try to explain how number of processors and problem size seem to
     affect runtimes/speedup in the problem. Consider that most Mesabi
     Nodes have 24 cores on them. Comment on whether this number seems
     to affect the performance of the runs you see.
Increasing the number of processors decreases the runtime to a point, then starts to slowly increase the runtime if there’s too many processors. This is likely due to the overhead caused by having additional processors. 

The problem size increasing also increases the runtime required for the problem. This makes sense because having more data means that there will be more time required to process all of it. 

Most Mesabi nodes have 24 cores on them. This does not appear to affect the performance of the runs, as I can tell. 




Problem 2: kmeans_serial vs kmeans_mpi
======================================

  Discuss how you chose to parallelize your serial version of K-means in
  the program `kmeans_mpi.c'. Answer the following questions briefly.

  1. How is the input and output data partitioned among the processors?
Firstly, we start off with our data file which contains a data point with a label and a feature vector. We only need to scatter the feature vectors among the various processors using a scatterv. After local computation is done we do an all-to-one communication via an all-gather to the root node. Both the input and output partition are considered row-wise partitioning because each feature is considered a row. 

The output is partitioned among the processors by rows as well. Since the input data is split by rows to the processors, each processor’s output will also be split up by rows. Each processor’s output will be combined to create the final output. 


  2. What communication is required throughout the algorithm?
Only the root processor reads in the datafile, so we have to broadcast the number of data and the dimension to all other processors. These are used to initialize their own local cluster and prepare for the scatterv, which scatters the features read in from the root proc. After every proc gets scattered some partition of the original features, we move onto the main algorithm where we first calculate the initial assignments for all our features and the local sum. This does not have to be communicated. After we calculate this local sum we have to allreduce among all the processors our local_clust features, to allow for every processor to do their own division step. Then each processor decides the best cluster for their features and we have to do another allreduce on the cluster counts to allow for the root processor to print out the number of changes, nchanges. In addition, we have to do a allreduce on nchanges to allow for all procs to terminate their cluster assignment algorithm at the same time. Finally, after we determine all the optimal cluster assignments we have to do 1 final gatherv call to give the root proc all the cluster assignments which were decided for all the features each proc has.  

Overall, the algorithm requires frequent communication when performing the clustering. This involves sharing the cluster data, the local sums, and then making sure the changes are saved. Additionally, the algorithm also requires communication to ensure all of the local data/local clusters are combined and gathered back into the root processor. 


  3. Which MPI collective communication operations did you employ?
Several MPI collective communication operations were useful in this situation.
Broadcast
Scatter (vector)
All-reduce
Gather

Broadcast was used as a way to distribute the initial data in this problem, such as the dimension size and the number of data. 

Scatter was used as a way to distribute the data to the various processors, splitting them by row. The vector version of scatter was used because there may be an uneven distribution of the data. 

All-reduce was used in a few different places. It’s used to synchronize the local clusters, local counts, and to ensure  That the termination condition for the clustering algorithm stops at the same time for all procs.

Gather is used to collect all of the data into the root processor for each iteration. This includes the local features and the local assignments during the clustering. 



Problem 3: kmeans_mpi
=====================

kmeans_mpi Timing Table
~~~~~~~~~~~~~~~~~~~~~~~

  Fill in the following table on measuring the performance of your
  `kmeans_mpi' program on MSI's Mesabi machine. Replace 00.00 entries
  with your actual run times. You can use the provided `kmeans-slurm.sh'
  script to ease this task.

  The columns are for each of 3 data files that are provided and run in
  the job script.

  digits_all_5e3.txt digits_all_1e4.txt
  -------------------------------------------------------------------
                                       Data File                     
   Procs  digits_all_5e3.txt  digits_all_1e4.txt  digits_all_3e4.txt 
  -------------------------------------------------------------------
       1               12.56               28.33               5.86
       2               10.86               12.92               4.89 
       4               8.51                16.63               4.93
       8               6.70                8.32                5.00 
      10               6.86                10.82               5.02 
      16               7.71                8.24                5.11 
      32               6.86                8.93                7.24 
      64               8.68                11.34               7.58 
     128               18.04               13.45               7.13 
  -------------------------------------------------------------------


kmeans_mpi Discussion Questions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Analyze your table of results and answer the following questions.
  1. Did using more processors result in speedups?
Once again, using more processors appeared to result in a speedup. However, only to a certain point, and some of the speedups are not as noticeable. For example, having 2 processors instead of a singular processor resulted in a speedup in all listed problems. In many other instances, the speedup resulting from having more processors was not very noticeable, or even slowed down. For example, moving from 10 to 16 processors had some problems solved faster with more processors (10.82 vs 8.24 on digits_all_1e4.txt), while some was slower (6.86 vs 7.71 on digits_all_5e3.txt). 


  2. Describe any trends or anomalies you see in the timings and
     speculate on their causes - e.g. was there are a steady increase in
     runtimes, steady decrease, or jagged changes in timing?
A trend that I noticed was that at first, having more processors was correlated to having a lower runtime. However, after a certain point, increasing the amount of processors usually ended up increasing the processing time. I would speculate that this is due to the overhead required with having more processors. 

Additionally, the changes overall seemed to be pretty jagged. I would guess this is due to the processors not giving too much of a speedup, which leads to variability in the results. 


  3. Try to explain how number of processors and problem size seem to
     affect runtimes/speedup in the problem. Consider that most Mesabi
     Nodes have 24 cores on them. Comment on whether this number seems
     to affect the performance of the runs you see.
In general, increasing the number of processors is loosely associated with decreasing the runtime. However, the more processors there are the slower the speedup increases. In other words, more processors past a certain point (e.g. 64 processors in this case) seems to increase the runtime.

Additionally, the smaller the problem size including the local data and the dimensions, the less time it takes for the program to run. Which makes sense, since there is less data for the processors to compute. 

Most Mesabi nodes have 24 cores on them. This does not appear to affect the performance of the runs, as far as I can tell.
